# -*- coding: utf-8 -*-
"""DM-project2-netflix.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q9cd1vaUknTjKT-GzfWPgnPqR7aPrlWZ

# **import essential libraries**
"""

from google.colab import drive
import zipfile
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import requests
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer, WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
!pip install transformers
from transformers import BertTokenizer, BertModel
import torch

"""# **Read the Dataset**"""

drive.mount('/content/drive')

dataset_path = '/content/drive/MyDrive/netflix.csv'
df = pd.read_csv(dataset_path)

"""# **Preprocessing**

**analyze dataset**
"""

def analyzeDataset():
    missing_values = df.isnull().sum()
    fig, ax = plt.subplots(figsize=(12, 6))
    missing_values.plot.bar(ax=ax)
    for i, val in enumerate(missing_values):
        ax.text(i, val + 10, str(val), ha='center', fontweight='bold')

    plt.show()

analyzeDataset()

"""**delete useless columns**"""

df.drop('date_added', axis=1, inplace=True)
df.drop('rating', axis=1, inplace=True)
df.drop('country', axis=1, inplace=True)

"""**fill missing data with forward fill**"""

df['duration'].fillna(method="ffill", inplace=True)

"""**delete bullshit rows**

counting the number of records which both 'cast' and 'director' columns are empty.
"""

counter = 0
for index, row in df.iterrows():
  if pd.isnull(row['cast']) and pd.isnull(row['director']):
    counter += 1

counter  #352 item

df.dropna(subset=['director'], how='all', inplace=True)

"""**get missing data from OMDB API**

in this section, we use the api of OMDB website to get the informatio of a show/movie. but since there's a limit on api requests (1000 requests per day), we couldn't use this method.
"""

api_key = '879de04b'
counter = 0
for index, row in df.iterrows():
  if counter == 1000:
    break
  if pd.isnull(row['country']) or pd.isnull(row['cast']) or pd.isnull(row['director']):
    title = row['title']
    url = f'http://www.omdbapi.com/?apikey={api_key}&t={title}'

    response = requests.get(url)
    if response.status_code == 200:
      movie_data = response.json()
      if pd.isnull(row['cast']) and 'Actors' in movie_data:
        df.at[index, 'cast'] = movie_data['Actors']
      if pd.isnull(row['director']) and 'Director' in movie_data:
        df.at[index, 'director'] = movie_data['Director']

  counter += 1

"""**plot outliers**"""

numeric_fields = ['release_year']

plt.figure(figsize=(8, 6))

# Boxplot for each numeric field
for field in numeric_fields:
    boxplot = plt.boxplot(df[field], vert=False, labels=[field], patch_artist=True)
    box_color = 'orange'
    for box in boxplot['boxes']:
        box.set_facecolor(box_color)
    plt.setp(boxplot['whiskers'], color='black')
    plt.setp(boxplot['caps'], color='black')
    plt.setp(boxplot['medians'], color='black')

    for flier in boxplot['fliers']:
        flier.set(marker='x', alpha=0.8)

    plt.title(f'Outliers for {field}')
    plt.show()

"""**delete outliers based on release years**"""

for field in numeric_fields:
    q1 = df[field].quantile(0.25)
    q3 = df[field].quantile(0.75)
    iqr = q3 - q1
    threshold = 1.5 * iqr

    lower_bound = q1 - threshold
    upper_bound = q3 + threshold
    print(lower_bound, upper_bound)

    # field_outliers = df[(df[field] < lower_bound) | (df[field] > upper_bound)]

    df = df[(df[field] >= lower_bound) & (df[field] <= upper_bound)]


# field_outliers.shape    #718 - (low: 2003, upper: 2028)

df['release_year'].min()

"""**convert numerical to categorical**"""

bins = [1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000, 2010, 2020, 2030]
labels = ['20s', '30s', '40s', '50s', '60s', '70s', '80s', '90s', '2000s', '2010s', '2020s']

df['release_year'] = pd.cut(df['release_year'], bins=bins, labels=labels)

"""**plot release year**"""

decade_counts = df['release_year'].value_counts().sort_index()

# Define colors for each bar
colors = ['lightblue', 'lightgreen', 'lightcoral', 'lightpink', 'lightsalmon', 'yellow',
          'lightcyan', 'lightgrey', 'lightgreen', 'lightblue', 'lightcoral']

plt.figure(figsize=(10, 6))
bars = plt.bar(decade_counts.index, decade_counts.values, color=colors)

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width() / 2, yval, yval, ha='center', va='bottom')

plt.xlabel('Decade')
plt.ylabel('Number of Records')
plt.title('shows/movies based on release year')

plt.grid(axis='y', linestyle='--', alpha=0.5)

plt.show()

"""**preprocess text (stemmer, lemmatizer)**"""

nltk.download('stopwords')
nltk.download('wordnet')

stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    tokens = text.lower().split()  # Tokenization and lowercase
    tokens = [token for token in tokens if token not in stop_words]  # Stopword removal
    tokens = [stemmer.stem(token) for token in tokens]  # Stemming
    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatizing (alternative to stemming)
    return ' '.join(tokens)

text_fields = ['description']
for field in text_fields:
    df[field] = df[field].apply(preprocess_text)

print(df.loc[0, 'description'])

df['director'].fillna('unknown', inplace=True)
df['cast'].fillna('unknown', inplace=True)
# analyzeDataset()

director_counts = df['director'].value_counts()
average_films = director_counts.mean()
average_films

director_counts = df['director'].value_counts()
average_films = director_counts.mean()

director_stats = []

for director, count in director_counts.items():
    if count > average_films:
        status = 'Above Average'
    elif count < average_films:
        status = 'Below Average'
    else:
        status = 'Average'

    director_stats.append({
        'Director': director,
        'Number of Movies': count,
        'Above/Below Average': status
    })

director_stats = pd.concat([pd.DataFrame(director_stats)], ignore_index=True)

director_stats[650:]

# Calculate the counts of directors above and below average
above_average_count = len(director_stats[director_stats['Above/Below Average'] == 'Above Average'])
below_average_count = len(director_stats[director_stats['Above/Below Average'] == 'Below Average'])

bar_colors = ['lightgreen', 'lightpink']

plt.figure(figsize=(5, 6))
plt.bar(['Above Average', 'Below Average'], [above_average_count, below_average_count], color=bar_colors)
plt.xlabel('Above/Below Average')
plt.ylabel('Number of Directors')
plt.title('Proportion of Directors Above and Below Average')

plt.show()

"""# **Phase 2**"""

!pip install mlxtend

df['listed_in'].unique()

from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# Filter out rows with 'unknown' values in 'director' and 'cast' columns
filtered_df = df[(df['director'] != 'unknown') & (df['cast'] != 'unknown')]

subset_df = filtered_df[['director', 'cast', 'listed_in']]

# Convert the subset dataframe to a list of lists and convert values to strings
transactionsList = subset_df.astype(str).values.tolist()

transactions = []
for transaction in transactionsList:
    cast_list = transaction[1].split(', ')  # Split the cast names by comma and space
    listed_in_list = transaction[2].split(', ')  # Split the listed_in categories by comma and space
    transactions.append([transaction[0]] + cast_list + listed_in_list)

# Apply one-hot encoding to convert the transaction list to a transaction matrix
te = TransactionEncoder()
te_array = te.fit_transform(transactions)
transaction_matrix = pd.DataFrame(te_array, columns=te.columns_)

# Apply the Apriori algorithm to find frequent itemsets
frequent_itemsets = apriori(transaction_matrix, min_support=0.05, use_colnames=True)

# Display the frequent itemsets
frequent_itemsets

for index, row in frequent_itemsets.iterrows():
    itemset = row['itemsets']
    support = row['support']
    print(f"Itemset: {itemset}, Support: {support}")

rules = association_rules(frequent_itemsets, metric='confidence', min_threshold=0.5)

rules

"""# **Phase 3**"""

!pip install transformers
!pip install torch
!pip install transformers
!pip install sentence-transformers
from transformers import BertTokenizer, BertModel
import torch
from mlxtend.preprocessing import TransactionEncoder
from mlxtend.frequent_patterns import apriori, association_rules
import numpy as np
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sentence_transformers import SentenceTransformer
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression

model = SentenceTransformer('bert-base-uncased')
descriptions = df['description'].tolist()
description_vectors = model.encode(descriptions)
num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters)
cluster_labels = kmeans.fit_predict(description_vectors)
pca = PCA(n_components=2)
reduced_vectors = pca.fit_transform(description_vectors)
plt.scatter(reduced_vectors[:, 0], reduced_vectors[:, 1], c=cluster_labels)
plt.title('BERT Sentence Clustering')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

model_name = 'bert-base-uncased'  # Choose the desired BERT model, e.g., 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertModel.from_pretrained(model_name)
descriptions = df['description'].tolist()
encoded_inputs = tokenizer(descriptions, padding=True, truncation=True, max_length=512, return_tensors='pt')
with torch.no_grad():
    outputs = model(**encoded_inputs)

embeddings = outputs.last_hidden_state

# Add embeddings to the DataFrame
df['embeddings'] = embeddings.tolist()

from sklearn.decomposition import PCA

# Reshape 'embeddings' to have two dimensions
embeddings_reshaped = embeddings.reshape(embeddings.shape[0], -1)

# Apply PCA to reduce the dimensionality to two dimensions
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings_reshaped)

unique_listed_in = np.unique(df['listed_in'])

new_array = np.array([])

for item in unique_listed_in:
    split_items = item.split(', ')
    new_array = np.append(new_array, split_items)


unique_values = np.unique(new_array)
num_unique_values = len(unique_values)

print("Number of unique values:", num_unique_values)

unique_values

from sklearn.cluster import DBSCAN
from sklearn.manifold import TSNE

# Flatten the embeddings array to 2D
num_samples, num_features = embeddings.shape[0], np.prod(embeddings.shape[1:])
embeddings_2d = embeddings.reshape(num_samples, num_features)

# Reduce the dimensionality of 'embeddings' using t-SNE
tsne = TSNE(n_components=2)
embeddings_2d = tsne.fit_transform(embeddings_2d)

# Apply DBSCAN clustering
dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust the parameters as needed
clusters = dbscan.fit_predict(embeddings_2d)

# Plot the data points with different colors for each cluster
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='viridis')
plt.title('DBSCAN Clustering Results')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.colorbar(label='Cluster')
plt.show()

num_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)
print(f"Number of clusters: {num_clusters}")

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

# Reshape 'embeddings' to have two dimensions
embeddings_reshaped = embeddings.reshape(embeddings.shape[0], -1)

# Apply PCA to reduce the dimensionality to two dimensions
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings_reshaped)

# Perform K-means clustering on the reduced-dimensional data
kmeans = KMeans(n_clusters=11)
clusters = kmeans.fit_predict(embeddings_2d)

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Reshape 'embeddings' to have three dimensions
embeddings_reshaped = embeddings.reshape(embeddings.shape[0], -1)

# Apply PCA to reduce the dimensionality to three dimensions
pca = PCA(n_components=3)
embeddings_3d = pca.fit_transform(embeddings_reshaped)

# Visualize the clusters in 3D
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.scatter(embeddings_3d[:, 0], embeddings_3d[:, 1], embeddings_3d[:, 2], c=clusters, cmap='viridis')

# Set labels and title
ax.set_xlabel('Dimension 1')
ax.set_ylabel('Dimension 2')
ax.set_zlabel('Dimension 3')
ax.set_title('K-means Clustering Results')

plt.show()

# Plot the data points with different colors for each cluster
plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=clusters, cmap='viridis')
plt.title('Clustering Results')
plt.xlabel('Dimension 1')
plt.ylabel('Dimension 2')
plt.colorbar(label='Cluster')
plt.show()

"""**classification**"""

# Extract the first genre from the 'listed_in' column
df['label'] = df['listed_in'].str.split(',').str[0].str.strip()

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import numpy as np

# Prepare the Data
X = embeddings  # BERT embeddings for film descriptions
y = df['listed_in']  # Target Variable: Listed_in (Genre)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Classification Model (Logistic Regression)
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluate the Model
y_pred = model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Predict the Genre for a Film
film_description = "This film is a thrilling crime drama set in a futuristic city."
film_embedding = # Obtain BERT embedding for the film description
predicted_genre = model.predict(np.array([film_embedding]))
print("Predicted Genre:", predicted_genre)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.multioutput import MultiOutputClassifier
from sklearn.svm import LinearSVC
from sklearn.metrics import classification_report

# Assume you have a DataFrame called 'df' with 'description' and 'listed_in' columns
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(embeddings, df['listed_in'], test_size=0.2, random_state=42)

# Reshape the X_train array to have 2 dimensions
X_train_2d = X_train.reshape(X_train.shape[0], -1)

# Create the multi-label binarizer for the labels
mlb = MultiLabelBinarizer()
y_train_binary = mlb.fit_transform(y_train)
y_test_binary = mlb.transform(y_test)

# Train the multi-label classifier
classifier = MultiOutputClassifier(LinearSVC())
classifier.fit(X_train_2d, y_train_binary)

# Predict the genres for the test set
X_test_2d = X_test.reshape(X_test.shape[0], -1)
y_pred_binary = classifier.predict(X_test_2d)

# Convert the binary predictions back to genre labels
y_pred = mlb.inverse_transform(y_pred_binary)

# Evaluate the performance of the classifier
print(classification_report(y_test, y_pred))

import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from transformers import BertTokenizer, BertModel

# Extract the first genre from the 'listed_in' column
df['label'] = df['listed_in'].str.split(',').str[0].str.strip()

# Prepare data for classification
X = embeddings  # Input features (BERT embeddings)
y = df['label']  # Target variable (genre label)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))

# Extract the first genre from the 'listed_in' column
df['label'] = df['listed_in'].str.split(',').str[0].str.strip()

df[:10]

import torch
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from transformers import BertTokenizer, BertModel

# Prepare data for classification
X = np.array(embeddings.tolist())  # Input features (BERT embeddings)
y = df['label']  # Target variable (genre label)

# Apply dimensionality reduction using PCA
pca = PCA(n_components=50)  # Specify the desired number of components
X_reduced = pca.fit_transform(X)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.2, random_state=42)

# Train a logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Evaluate the model
y_pred = model.predict(X_test)
print(classification_report(y_test, y_pred))